{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":70942,"databundleVersionId":10381525},{"sourceType":"kernelVersion","sourceId":221901410},{"sourceType":"kernelVersion","sourceId":221902053},{"sourceType":"kernelVersion","sourceId":223106941},{"sourceType":"kernelVersion","sourceId":224725622}],"dockerImageVersionId":30887,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom torch.utils.data import TensorDataset\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\n\n\ndef get_X_cat(df, cat_cols, transformers=None):\n    \"\"\"\n    Apply a specific categorical data transformer or a LabelEncoder if None.\n    \"\"\"\n    if transformers is None:\n        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n    return transformers, np.array(\n        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n    ).T\n\n\ndef preprocess_data(train, val):\n    \"\"\"\n    Standardize numerical variables and transform (Label-encode) categoricals.\n    Fill NA values with mean for numerical.\n    Create torch dataloaders to prepare data for training and evaluation.\n    \"\"\"\n    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n    scaler = StandardScaler()\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n    X_num_train = imp.fit_transform(train[numerical])\n    X_num_train = scaler.fit_transform(X_num_train)\n    X_num_val = imp.transform(val[numerical])\n    X_num_val = scaler.transform(X_num_val)\n    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n    dl_val = init_dl(X_cat_val, X_num_val, val)\n    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n\n\ndef get_categoricals(train, val):\n    \"\"\"\n    Remove constant categorical columns and transform them using LabelEncoder.\n    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n    \"\"\"\n    categorical_cols, numerical = get_feature_types(train)\n    remove = []\n    for col in categorical_cols:\n        if train[col].nunique() == 1:\n            remove.append(col)\n        ind = ~val[col].isin(train[col])\n        if ind.any():\n            val.loc[ind, col] = np.nan\n    categorical_cols = [col for col in categorical_cols if col not in remove]\n    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n    return X_cat_train, X_cat_val, numerical, transformers\n\n\ndef init_dl(X_cat, X_num, df, training=False):\n    \"\"\"\n    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n    Notice that efs_time is log-transformed.\n    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n    \"\"\"\n    ds_train = TensorDataset(\n        torch.tensor(X_cat, dtype=torch.long),\n        torch.tensor(X_num, dtype=torch.float32),\n        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n        torch.tensor(df.efs.values, dtype=torch.long)\n    )\n    bs = 2048\n    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n    return dl_train\n\n\ndef get_feature_types(train):\n    \"\"\"\n    Utility function to return categorical and numerical column names.\n    \"\"\"\n    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n    FEATURES = [c for c in train.columns if not c in RMV]\n    numerical = [i for i in FEATURES if i not in categorical_cols]\n    return categorical_cols, numerical\n\n\ndef add_features(df):\n    \"\"\"\n    Create some new features to help the model focus on specific patterns.\n    \"\"\"\n    # sex_match = df.sex_match.astype(str)\n    # sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n    # df['sex_match_bool'] = sex_match\n    # df.loc[df.sex_match.isna(), 'sex_match_bool'] = np.nan\n    # df['big_age'] = df.age_at_hct > 16\n    # df.loc[df.year_hct == 2019, 'year_hct'] = 2020\n    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n    # df['strange_age'] = df.age_at_hct == 0.044\n    # df['age_bin'] = pd.cut(df.age_at_hct, [0, 0.0441, 16, 30, 50, 100])\n    # df['age_ts'] = df.age_at_hct / df.donor_age\n    df['year_hct'] -= 2000\n    \n    return df\n\n\ndef load_data():\n    \"\"\"\n    Load data and add features.\n    \"\"\"\n    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n    test = add_features(test)\n    print(\"Test shape:\", test.shape)\n    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n    train = add_features(train)\n    print(\"Train shape:\", train.shape)\n    return test, train","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:00:06.995709Z","iopub.execute_input":"2025-03-01T02:00:06.996095Z","iopub.status.idle":"2025-03-01T02:00:12.558925Z","shell.execute_reply.started":"2025-03-01T02:00:06.996060Z","shell.execute_reply":"2025-03-01T02:00:12.558155Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import functools\nfrom typing import List\n\nimport pytorch_lightning as pl\nimport numpy as np\nimport torch\nfrom lifelines.utils import concordance_index\nfrom pytorch_lightning.cli import ReduceLROnPlateau\nfrom pytorch_tabular.models.common.layers import ODST\nfrom torch import nn\nfrom pytorch_lightning.utilities import grad_norm\n\nclass CatEmbeddings(nn.Module):\n    \"\"\"\n    Embedding module for the categorical dataframe.\n    \"\"\"\n    def __init__(\n        self,\n        projection_dim: int,\n        categorical_cardinality: List[int],\n        embedding_dim: List[int] # Alterado para List[int]\n    ):\n        \"\"\"\n        projection_dim: The dimension of the final output after projecting the concatenated embeddings.\n        categorical_cardinality: A list of cardinalities for each categorical feature.\n        embedding_dim: A LIST of embedding dimensions for each categorical feature, corresponding to categorical_cardinality.\n        self.embeddings: list of embedding layers, now with different embedding dimensions.\n        self.projection: Sequential network for projecting concatenated embeddings.\n        \"\"\"\n        super(CatEmbeddings, self).__init__()\n\n        # Verificação importante: garantir que categorical_cardinality e embedding_dim têm o mesmo tamanho\n        if len(categorical_cardinality) != len(embedding_dim):\n            raise ValueError(\"categorical_cardinality and embedding_dim lists must have the same length.\")\n\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cardinality, dim) # Usando 'dim' para cada embedding dimension específica\n            for cardinality, dim in zip(categorical_cardinality, embedding_dim) # Iterando sobre ambas as listas juntas\n        ])\n        self.projection = nn.Sequential(\n            nn.Linear(sum(embedding_dim), projection_dim), # Ajustado para usar a soma das embedding_dim como dimensão de entrada\n            nn.GELU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n\n    def forward(self, x_cat):\n        \"\"\"\n        Apply the projection on concatened embeddings that contains all categorical features.\n        \"\"\"\n        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=1)\n        return self.projection(x_cat)\n\n\nclass NN(nn.Module):\n    \"\"\"\n    Train a model on both categorical embeddings and numerical data.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            dropout: float = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous features.\n        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n        projection_dim: The size of the projected output space for the categorical embeddings.\n        hidden_dim: The number of neurons in the hidden layer of the MLP.\n        dropout: The dropout rate applied in the network.\n        self.embeddings: previous embeddings for categorical data.\n        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n        self.out: linear output layer that maps the output of the MLP to a single value\n        self.dropout: defines dropout\n        Weights initialization with xavier normal algorithm and biases with zeros.\n        \"\"\"\n        super(NN, self).__init__()\n        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n        self.mlp = nn.Sequential(\n            ODST(projection_dim + continuous_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(dropout)\n        )\n        self.out = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        # initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Create embedding layers for categorical data, concatenate with continous variables.\n        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n        \"\"\"\n        x = self.embeddings(x_cat)\n        x = torch.cat([x, x_cont], dim=1)\n        x = self.dropout(x)\n        x = self.mlp(x)\n        return self.out(x), x\n\n\n@functools.lru_cache\ndef combinations(N):\n    \"\"\"\n    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n    and caches the result using functools.lru_cache for optimization\n    \"\"\"\n    ind = torch.arange(N)\n    comb = torch.combinations(ind, r=2)\n    return comb.cuda()\n\n\nclass LitNN(pl.LightningModule):\n    \"\"\"\n    Main Model creation and losses definition to fully train the model.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            lr: float = 1e-3,\n            dropout: float = 0.2,\n            weight_decay: float = 1e-3,\n            aux_weight: float = 0.1,\n            margin: float = 0.5,\n            race_index: int = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous input features.\n        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n        embedding_dim: The dimension of the embeddings for the categorical features.\n        projection_dim: The dimension of the projected space after embedding concatenation.\n        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n        lr: The learning rate for the optimizer.\n        dropout: Dropout probability to avoid overfitting.\n        weight_decay: The L2 regularization term for the optimizer.\n        aux_weight: Weight used for auxiliary tasks.\n        margin: Margin used in some loss functions.\n        race_index: An index that refer to race_group in the input data.\n        \"\"\"\n        super(LitNN, self).__init__()\n        self.save_hyperparameters()\n\n        # Creates an instance of the NN model defined above\n        self.model = NN(\n            continuous_dim=self.hparams.continuous_dim,\n            categorical_cardinality=self.hparams.categorical_cardinality,\n            embedding_dim=self.hparams.embedding_dim,\n            projection_dim=self.hparams.projection_dim,\n            hidden_dim=self.hparams.hidden_dim,\n            dropout=self.hparams.dropout\n        )\n        self.targets = []\n\n        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n        self.aux_cls = nn.Sequential(\n            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n            nn.GELU(),\n            nn.Linear(self.hparams.hidden_dim // 3, 1)\n        )\n\n    def on_before_optimizer_step(self, optimizer):\n        \"\"\"\n        Compute the 2-norm for each layer\n        If using mixed precision, the gradients are already unscaled here\n        \"\"\"\n        norms = grad_norm(self.model, norm_type=2)\n        self.log_dict(norms)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n        \"\"\"\n        x, emb = self.model(x_cat, x_cont)\n        return x.squeeze(1), emb\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        defines how the model processes each batch of data during training.\n        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n        Calculates loss and race_group loss on full data.\n        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n        Returns loss and aux_loss multiplied by weight defined above.\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        aux_pred = self.aux_cls(emb).squeeze(1)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n        aux_mask = efs == 1\n        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        return loss + aux_loss * self.hparams.aux_weight\n\n    def get_full_loss(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Output loss and race_group loss.\n        \"\"\"\n        loss = self.calc_loss(y, y_hat, efs)\n        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n        loss += 0.1 * race_loss\n        return loss, race_loss\n\n    def get_race_losses(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Calculate loss for each race_group based on deviation/variance.\n        \"\"\"\n        races = torch.unique(x_cat[:, self.hparams.race_index])\n        race_losses = []\n        for race in races:\n            ind = x_cat[:, self.hparams.race_index] == race\n            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n        race_loss = sum(race_losses) / len(race_losses)\n        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n        return torch.sqrt(races_loss_std)\n\n    def calc_loss(self, y, y_hat, efs):\n        \"\"\"\n        Most important part of the model : loss function used for training.\n        We face survival data with event indicators along with time-to-event.\n\n        This function computes the main loss by the following the steps :\n        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n        * make sure that we have at least 1 event in each pair\n        * convert y to +1 or -1 depending on the correct ranking\n        * loss is computed using a margin-based hinge loss\n        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n        * average loss on all pairs is returned\n        \"\"\"\n        N = y.shape[0]\n        comb = combinations(N)\n        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n        pred_left = y_hat[comb[:, 0]]\n        pred_right = y_hat[comb[:, 1]]\n        y_left = y[comb[:, 0]]\n        y_right = y[comb[:, 1]]\n        y = 2 * (y_left > y_right).int() - 1\n        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n        mask = self.get_mask(comb, efs, y_left, y_right)\n        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n        return loss\n\n    def get_mask(self, comb, efs, y_left, y_right):\n        \"\"\"\n        Defines all invalid comparisons :\n        * Case 1: \"Left outlived Right\" but Right is censored\n        * Case 2: \"Right outlived Left\" but Left is censored\n        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n        \"\"\"\n        left_outlived = y_left >= y_right\n        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n        mask2 = (left_outlived & left_1_right_0)\n        right_outlived = y_right >= y_left\n        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n        mask2 |= (right_outlived & right_1_left_0)\n        mask2 = ~mask2\n        mask = mask2\n        return mask\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        This method defines how the model processes each batch during validation\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"\n        At the end of the validation epoch, it computes and logs the concordance index\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n    def _calc_cindex(self):\n        \"\"\"\n        Calculate c-index accounting for each race_group or global.\n        \"\"\"\n        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n        metric = self._metric(efs, races, y, y_hat)\n        cindex = concordance_index(y, y_hat, efs)\n        return cindex, metric\n\n    def _metric(self, efs, races, y, y_hat):\n        \"\"\"\n        Calculate c-index accounting for each race_group\n        \"\"\"\n        metric_list = []\n        for race in np.unique(races):\n            y_ = y[races == race]\n            y_hat_ = y_hat[races == race]\n            efs_ = efs[races == race]\n            metric_list.append(concordance_index(y_, y_hat_, efs_))\n        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n        return metric\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Same as training step but to log test data\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def on_test_epoch_end(self) -> None:\n        \"\"\"\n        At the end of the test epoch, calculates and logs the concordance index for the test set\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        configures the optimizer and learning rate scheduler:\n        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        scheduler_config = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=45,\n                eta_min=6e-3\n            ),\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n            \"strict\": False,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:00:12.560327Z","iopub.execute_input":"2025-03-01T02:00:12.560686Z","iopub.status.idle":"2025-03-01T02:00:33.309168Z","shell.execute_reply.started":"2025-03-01T02:00:12.560663Z","shell.execute_reply":"2025-03-01T02:00:33.308378Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import json\nimport pytorch_lightning as pl\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\nfrom pytorch_lightning.callbacks import StochasticWeightAveraging\nfrom sklearn.model_selection import StratifiedKFold\n\npl.seed_everything(42)\n\ndef main(hparams):\n    \"\"\"\n    Main function to train the model.\n    The steps are as following :\n    * load data and fill efs and efs time for test data with 1\n    * initialize pred array with 0\n    * get categorical and numerical columns\n    * split the train data on the stratified criterion : race_group * newborns yes/no\n    * preprocess the fold data (create dataloaders)\n    * train the model and create final submission output\n    \"\"\"\n    test, train_original = load_data()\n    test['efs_time'] = 1\n    test['efs'] = 1\n    test_pred = np.zeros(test.shape[0])\n    categorical_cols, numerical = get_feature_types(train_original)\n    kf = StratifiedKFold(n_splits=5, shuffle=True, )\n    for i, (train_index, test_index) in enumerate(\n        kf.split(\n            train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str)\n        )\n    ):\n        tt = train_original.copy()\n        train = tt.iloc[train_index]\n        val = tt.iloc[test_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols)\n        # Create submission\n        train = tt.iloc[train_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n        pred, _ = model.cuda().eval()(\n            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n        )\n        test_pred += pred.detach().cpu().numpy()\n        \n    subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n    subm_data['prediction'] = -test_pred\n    subm_data.to_csv('submission.csv', index=False)\n    \n    display(subm_data.head())\n    return \n\n\n\ndef train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n    \"\"\"\n    Defines model hyperparameters and fit the model.\n    \"\"\"\n    if hparams is None:\n        hparams = {\n            # \"embedding_dim\": 16,\n            \"projection_dim\": 112,\n            \"hidden_dim\": 56,\n            \"lr\": 0.06464861983337984,\n            \"dropout\": 0.05463240181423116,\n            \"aux_weight\": 0.26545778308743806,\n            \"margin\": 0.2588153271003354,\n            \"weight_decay\": 0.0002773544957610778\n        }\n    model = LitNN(\n        continuous_dim=X_num_train.shape[1],\n        categorical_cardinality=[len(t.classes_) for t in transformers],\n        embedding_dim = [min(len(transformer.classes_), 16) for transformer in transformers],\n        race_index=categorical_cols.index(\"race_group\"),\n        **hparams\n    )\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n    trainer = pl.Trainer(\n        accelerator='cuda',\n        max_epochs=55,\n        log_every_n_steps=6,\n        callbacks=[\n            checkpoint_callback,\n            LearningRateMonitor(logging_interval='epoch'),\n            TQDMProgressBar(),\n            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=40, annealing_epochs=15)\n        ],\n    )\n    trainer.fit(model, dl_train)\n    trainer.test(model, dl_val)\n    return model.eval()\n\n\nhparams = None\nres = main(hparams)\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:00:33.312550Z","iopub.execute_input":"2025-03-01T02:00:33.312775Z","iopub.status.idle":"2025-03-01T02:08:50.132688Z","shell.execute_reply.started":"2025-03-01T02:00:33.312756Z","shell.execute_reply":"2025-03-01T02:08:50.131903Z"}},"outputs":[{"name":"stdout","text":"Test shape: (3, 59)\nTrain shape: (28800, 61)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9494a503dd7409b9d47f179b17ed0d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8416c755ae914d9980bc52cce9107872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m       test_cindex       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6791375279426575    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m   test_cindex_simple    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.692509651184082    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1820104595031154    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_cindex        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6791375279426575     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">    test_cindex_simple     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.692509651184082     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1820104595031154     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f12bbf5c81406ba2b05376442d7a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9f01c3cab6e47d6ad512e66b2f369fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m       test_cindex       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.674964189529419    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m   test_cindex_simple    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6846877336502075    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.18572098264832973   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_cindex        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.674964189529419     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">    test_cindex_simple     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6846877336502075     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.18572098264832973    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e824ba535d45e1bcf09c2963083bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5957aaf4d6446bbe066063564d23f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m       test_cindex       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6834500432014465    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m   test_cindex_simple    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6917163133621216    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.18309281894680524   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_cindex        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6834500432014465     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">    test_cindex_simple     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6917163133621216     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.18309281894680524    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1394a023ad4f0099ac89567378456b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4cfe873796d4c0a9ff2a5fbb6a64ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m       test_cindex       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6846335530281067    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m   test_cindex_simple    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6977168321609497    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.17873627677211423   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_cindex        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6846335530281067     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">    test_cindex_simple     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6977168321609497     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.17873627677211423    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c94ff1d8e94aca8d7aa49e2d1ace88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9285398536242f1a1bff48897626495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m       test_cindex       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6725793480873108    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m   test_cindex_simple    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.679293692111969    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.189214972077878    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">        test_cindex        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6725793480873108     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">    test_cindex_simple     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.679293692111969     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.189214972077878     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"      ID  prediction\n0  28800   -2.303652\n1  28801    0.060786\n2  28802   -3.015838","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28800</td>\n      <td>-2.303652</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28801</td>\n      <td>0.060786</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28802</td>\n      <td>-3.015838</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":3}]}